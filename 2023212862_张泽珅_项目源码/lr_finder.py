import os
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import transforms, models
from sklearn.model_selection import train_test_split
from PIL import Image
from tqdm import tqdm
import warnings
import matplotlib
matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False
warnings.filterwarnings('ignore')

# ä½¿ç”¨train_enhanced.pyçš„é…ç½®
from enhanced_ensemble_85 import CONFIG, SceneDataset, get_transforms, get_model

class LRFinder:
    """å­¦ä¹ ç‡æŸ¥æ‰¾å™¨ - æ‰¾åˆ°æœ€ä¼˜å­¦ä¹ ç‡"""
    
    def __init__(self, model, optimizer, criterion, device):
        self.model = model
        self.optimizer = optimizer
        self.criterion = criterion
        self.device = device
        
        self.history = {'lr': [], 'loss': []}
        self.best_lr = None
    
    def range_test(self, train_loader, start_lr=1e-7, end_lr=10, num_iter=100, 
                   smooth_f=0.05, diverge_th=5):
        """
        æ‰§è¡Œå­¦ä¹ ç‡èŒƒå›´æµ‹è¯•
        
        å‚æ•°:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            start_lr: èµ·å§‹å­¦ä¹ ç‡
            end_lr: ç»“æŸå­¦ä¹ ç‡
            num_iter: è¿­ä»£æ¬¡æ•°
            smooth_f: å¹³æ»‘å› å­
            diverge_th: å‘æ•£é˜ˆå€¼
        """
        # ä¿å­˜æ¨¡å‹åˆå§‹çŠ¶æ€
        model_state = self.model.state_dict()
        optimizer_state = self.optimizer.state_dict()
        
        # åˆå§‹åŒ–
        self.model.train()
        self.history = {'lr': [], 'loss': []}
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨ - æŒ‡æ•°å¢é•¿
        lr_schedule = np.geomspace(start_lr, end_lr, num_iter)
        
        iterator = iter(train_loader)
        smoothed_loss = 0
        best_loss = float('inf')
        batch_num = 0
        
        print(f"{'='*70}")
        print("å­¦ä¹ ç‡æŸ¥æ‰¾å™¨è¿è¡Œä¸­...")
        print(f"èŒƒå›´: {start_lr:.2e} â†’ {end_lr:.2e}")
        print(f"è¿­ä»£æ¬¡æ•°: {num_iter}")
        print(f"{'='*70}\n")
        
        progress_bar = tqdm(range(num_iter), desc="LRæŸ¥æ‰¾")
        
        for iteration in progress_bar:
            try:
                inputs, targets = next(iterator)
            except StopIteration:
                iterator = iter(train_loader)
                inputs, targets = next(iterator)
            
            # è®¾ç½®å­¦ä¹ ç‡
            lr = lr_schedule[iteration]
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = lr
            
            # å‰å‘ä¼ æ’­
            inputs, targets = inputs.to(self.device), targets.to(self.device)
            self.optimizer.zero_grad()
            
            outputs = self.model(inputs)
            loss = self.criterion(outputs, targets)
            
            # åå‘ä¼ æ’­
            loss.backward()
            self.optimizer.step()
            
            # è®°å½•
            self.history['lr'].append(lr)
            self.history['loss'].append(loss.item())
            
            # å¹³æ»‘æŸå¤±
            if iteration == 0:
                smoothed_loss = loss.item()
            else:
                smoothed_loss = smooth_f * loss.item() + (1 - smooth_f) * smoothed_loss
            
            # æ£€æŸ¥æ˜¯å¦å‘æ•£
            if smoothed_loss < best_loss:
                best_loss = smoothed_loss
            
            if smoothed_loss > diverge_th * best_loss:
                print(f"\n\nè®­ç»ƒå‘æ•£ï¼Œåœæ­¢æµ‹è¯•")
                break
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'lr': f'{lr:.2e}',
                'loss': f'{smoothed_loss:.4f}'
            })
            
            batch_num += 1
        
        # æ¢å¤æ¨¡å‹çŠ¶æ€
        self.model.load_state_dict(model_state)
        self.optimizer.load_state_dict(optimizer_state)
        
        print(f"\n\n{'='*70}")
        print("å­¦ä¹ ç‡æŸ¥æ‰¾å®Œæˆ!")
        print(f"{'='*70}")
    
    def plot(self, skip_start=10, skip_end=5, log_lr=True, save_path='lr_finder.png'):
        """
        ç»˜åˆ¶å­¦ä¹ ç‡ vs æŸå¤±æ›²çº¿
        
        å‚æ•°:
            skip_start: è·³è¿‡å¼€å§‹çš„Nä¸ªç‚¹
            skip_end: è·³è¿‡ç»“æŸçš„Nä¸ªç‚¹
            log_lr: æ˜¯å¦ä½¿ç”¨å¯¹æ•°åæ ‡
            save_path: ä¿å­˜è·¯å¾„
        """
        if not self.history['lr']:
            print("é”™è¯¯: éœ€è¦å…ˆè¿è¡Œrange_test()")
            return
        
        # æˆªå–æ•°æ®
        lrs = self.history['lr'][skip_start:-skip_end if skip_end > 0 else None]
        losses = self.history['loss'][skip_start:-skip_end if skip_end > 0 else None]
        
        # æ‰¾åˆ°æœ€å°æŸå¤±å¯¹åº”çš„å­¦ä¹ ç‡
        min_loss_idx = losses.index(min(losses))
        min_loss_lr = lrs[min_loss_idx]
        
        # æ‰¾åˆ°æ¢¯åº¦æœ€å¤§çš„å­¦ä¹ ç‡ï¼ˆæ¨èï¼‰
        # é€šè¿‡è®¡ç®—æŸå¤±çš„ä¸€é˜¶å¯¼æ•°
        grad = np.gradient(losses)
        max_grad_idx = np.argmin(grad)  # æœ€è´Ÿçš„æ¢¯åº¦
        suggested_lr = lrs[max_grad_idx]
        
        # æ¨èå­¦ä¹ ç‡ä¸ºæœ€å¤§æ¢¯åº¦ç‚¹çš„1/10
        self.best_lr = suggested_lr / 10
        
        # ç»˜å›¾
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # å·¦å›¾: LR vs Loss
        ax1.plot(lrs, losses, linewidth=2)
        ax1.axvline(min_loss_lr, color='red', linestyle='--', 
                   label=f'æœ€å°Loss LR: {min_loss_lr:.2e}', linewidth=1.5)
        ax1.axvline(suggested_lr, color='green', linestyle='--',
                   label=f'æœ€å¤§æ¢¯åº¦ LR: {suggested_lr:.2e}', linewidth=1.5)
        ax1.axvline(self.best_lr, color='purple', linestyle='--',
                   label=f'æ¨è LR: {self.best_lr:.2e}', linewidth=2)
        
        if log_lr:
            ax1.set_xscale('log')
        ax1.set_xlabel('Learning Rate', fontsize=12)
        ax1.set_ylabel('Loss', fontsize=12)
        ax1.set_title('Learning Rate Finder', fontsize=14, fontweight='bold')
        ax1.legend(fontsize=10)
        ax1.grid(True, alpha=0.3)
        
        # å³å›¾: Lossçš„å˜åŒ–ç‡
        ax2.plot(lrs, grad, linewidth=2, color='orange')
        ax2.axvline(suggested_lr, color='green', linestyle='--',
                   label=f'æœ€å¤§æ¢¯åº¦ç‚¹: {suggested_lr:.2e}', linewidth=1.5)
        ax2.axhline(0, color='black', linestyle='-', alpha=0.3)
        
        if log_lr:
            ax2.set_xscale('log')
        ax2.set_xlabel('Learning Rate', fontsize=12)
        ax2.set_ylabel('Losså˜åŒ–ç‡ (æ¢¯åº¦)', fontsize=12)
        ax2.set_title('Loss Gradient', fontsize=14, fontweight='bold')
        ax2.legend(fontsize=10)
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"\nå›¾è¡¨å·²ä¿å­˜: {save_path}")
        plt.close()
        
        # æ‰“å°å»ºè®®
        print(f"\n{'='*70}")
        print("ğŸ“Š å­¦ä¹ ç‡åˆ†æç»“æœ")
        print(f"{'='*70}")
        print(f"æœ€å°æŸå¤±å¯¹åº”çš„å­¦ä¹ ç‡: {min_loss_lr:.2e}")
        print(f"æŸå¤±ä¸‹é™æœ€å¿«çš„å­¦ä¹ ç‡: {suggested_lr:.2e}")
        print(f"âœ… æ¨èå­¦ä¹ ç‡: {self.best_lr:.2e} (æœ€å¤§æ¢¯åº¦çš„1/10)")
        print(f"{'='*70}")
        print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
        print(f"1. åœ¨CONFIGä¸­è®¾ç½®: CONFIG['lr'] = {self.best_lr:.2e}")
        print(f"2. å¦‚æœæƒ³è¦æ›´æ¿€è¿›: CONFIG['lr'] = {suggested_lr:.2e}")
        print(f"3. å¦‚æœæƒ³è¦æ›´ä¿å®ˆ: CONFIG['lr'] = {self.best_lr / 3:.2e}")
        print(f"{'='*70}\n")
        
        return self.best_lr

def find_optimal_lr(model_name='resnet50', num_samples=2000):
    """
    ä¸ºæŒ‡å®šæ¨¡å‹æ‰¾åˆ°æœ€ä¼˜å­¦ä¹ ç‡
    
    å‚æ•°:
        model_name: æ¨¡å‹åç§°
        num_samples: ä½¿ç”¨çš„æ ·æœ¬æ•°é‡
    """
    print(f"{'='*70}")
    print(f"ä¸º {model_name} æŸ¥æ‰¾æœ€ä¼˜å­¦ä¹ ç‡")
    print(f"{'='*70}\n")
    
    # åŠ è½½æ•°æ®ï¼ˆä½¿ç”¨éƒ¨åˆ†æ•°æ®åŠ é€Ÿï¼‰
    all_paths = []
    all_labels = []
    for cls_name, idx in CONFIG['class_map'].items():
        cls_folder = os.path.join(CONFIG['data_root'], cls_name)
        if os.path.exists(cls_folder):
            imgs = [os.path.join(cls_folder, i) for i in os.listdir(cls_folder) 
                   if i.lower().endswith(('.jpg', '.png', '.jpeg'))]
            all_paths.extend(imgs)
            all_labels.extend([idx] * len(imgs))
    
    # åªä½¿ç”¨éƒ¨åˆ†æ•°æ®
    if len(all_paths) > num_samples:
        indices = np.random.choice(len(all_paths), num_samples, replace=False)
        all_paths = [all_paths[i] for i in indices]
        all_labels = [all_labels[i] for i in indices]
    
    print(f"ä½¿ç”¨ {len(all_paths)} å¼ å›¾ç‰‡è¿›è¡Œå­¦ä¹ ç‡æœç´¢\n")
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = SceneDataset(all_paths, all_labels, 'train', 
                          get_transforms('train', 'medium'))
    loader = DataLoader(dataset, batch_size=64, shuffle=True, 
                       num_workers=8, pin_memory=True)
    
    # åˆ›å»ºæ¨¡å‹
    model = get_model(model_name).to(CONFIG['device'])
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=1e-7, weight_decay=CONFIG['weight_decay'])
    
    # è¿è¡Œå­¦ä¹ ç‡æŸ¥æ‰¾
    lr_finder = LRFinder(model, optimizer, criterion, CONFIG['device'])
    lr_finder.range_test(loader, start_lr=1e-7, end_lr=1, num_iter=100)
    
    # ç»˜åˆ¶å¹¶è·å–æ¨èå­¦ä¹ ç‡
    best_lr = lr_finder.plot(save_path=f'lr_finder_{model_name}.png')
    
    return best_lr

def find_all_models_lr():
    """ä¸ºæ‰€æœ‰æ¨¡å‹æ‰¾åˆ°æœ€ä¼˜å­¦ä¹ ç‡"""
    results = {}
    
    for model_name in CONFIG['models']:
        print(f"\n\n{'#'*70}")
        print(f"# æ¨¡å‹: {model_name}")
        print(f"{'#'*70}\n")
        
        try:
            best_lr = find_optimal_lr(model_name)
            results[model_name] = best_lr
        except Exception as e:
            print(f"é”™è¯¯: {e}")
            results[model_name] = None
    
    # æ€»ç»“
    print(f"\n\n{'='*70}")
    print("ğŸ“Š æ‰€æœ‰æ¨¡å‹çš„æ¨èå­¦ä¹ ç‡æ±‡æ€»")
    print(f"{'='*70}")
    
    valid_lrs = [lr for lr in results.values() if lr is not None]
    if valid_lrs:
        avg_lr = np.mean(valid_lrs)
        
        for model_name, lr in results.items():
            if lr:
                print(f"{model_name:20s}: {lr:.2e}")
        
        print(f"\n{'='*70}")
        print(f"ğŸ“Œ å¹³å‡æ¨èå­¦ä¹ ç‡: {avg_lr:.2e}")
        print(f"ğŸ’¡ å»ºè®®åœ¨CONFIGä¸­è®¾ç½®: CONFIG['lr'] = {avg_lr:.2e}")
        print(f"{'='*70}\n")
        
        return avg_lr
    else:
        print("é”™è¯¯: æ²¡æœ‰æˆåŠŸçš„å­¦ä¹ ç‡æœç´¢")
        return None

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='å­¦ä¹ ç‡æŸ¥æ‰¾å™¨')
    parser.add_argument('--model', type=str, default='resnet50',
                       help='æ¨¡å‹åç§° (resnet50, efficientnet_b4, etc.)')
    parser.add_argument('--all', action='store_true',
                       help='ä¸ºæ‰€æœ‰æ¨¡å‹æŸ¥æ‰¾å­¦ä¹ ç‡')
    parser.add_argument('--samples', type=int, default=2000,
                       help='ä½¿ç”¨çš„æ ·æœ¬æ•°é‡')
    
    args = parser.parse_args()
    
    if args.all:
        find_all_models_lr()
    else:
        find_optimal_lr(args.model, args.samples)